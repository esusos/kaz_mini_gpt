{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec850cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from utils import set_seed\n",
    "from utils import sample\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3df13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b4144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        \n",
    "        print(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9707abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b78e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3493140 characters, 145 unique.\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'C', 'H', 'I', 'M', 'T', 'V', 'X', '^', 'a', 'b', 'c', 'e', 'f', 'h', 'i', 'l', 'n', 'o', 'p', 's', 't', 'y', '~', '£', '¥', '«', '»', 'Ё', 'І', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ч', 'Ш', 'Щ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', 'і', 'Ғ', 'ғ', 'Қ', 'қ', 'Ң', 'ң', 'Ү', 'ү', 'Ұ', 'ұ', 'Һ', 'һ', 'Ә', 'ә', 'Ө', 'ө', '–', '—', '„', '…', '№', '™']\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('kara_sozder_abay_joly.txt', 'r', encoding='utf-8').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2c54f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/12/2023 11:00:05 - INFO - model -   number of parameters: 2.524320e+05\n"
     ]
    }
   ],
   "source": [
    "from model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=3, n_head=3, n_embd=48)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab370b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=5, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685ed8b",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908fd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = 'gpt200k.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e767a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dff3ec",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919dfd78",
   "metadata": {},
   "source": [
    "# Error caused by tokens mismatching, text must be Abay zholy + kara_sozder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ac7d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/12/2023 11:00:07 - INFO - model -   number of parameters: 2.524320e+05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(145, 48)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (time_shift): ZeroPad2d((0, 0, 1, 0))\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (time_shift): ZeroPad2d((0, 0, 1, 0))\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (query): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (value): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "        (time_shift): ZeroPad2d((0, 0, 1, 0))\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=48, out_features=192, bias=True)\n",
       "        (1): GELU(approximate=none)\n",
       "        (2): Linear(in_features=192, out_features=48, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=48, out_features=145, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(mconf)\n",
    "model.load_state_dict(torch.load('gpt200k_abay.pt'))\n",
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a07692b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Абай өзінен үлкен кісінің қорыққанына қатты ырза еді. Жұмабайдың неге ашуланғанын ұғып тұр. Қоңыр жүзі қызарып, төмен қарап қысыла күле беріп, бөркін айналдыра бастады. Кәдімгі «жолбасар» ұрыларша шапан-бөркін айналдырып киіп, мұрны мен аузын қызыл орамалмен таңып алып, Жұмабайды қуғанда тағы сол ұрыларша, «даусымды танытпаймын» деп, мыңқылдап сөйлеп бұйрық берген. Байтас қорықса, қорықпаса да сыр алдырған жоқ Сондықтан, Жұмабайдың ашуын алыстан танып, мәз болып күліп келедi. Былай \n"
     ]
    }
   ],
   "source": [
    "context = \"Абай өзінен үлкен кісінің қорыққанына қатты ырза еді. Жұмабайдың неге ашуланғанын ұғып тұр. Қоңыр жүзі қызарып, төмен қарап қысыла күле беріп, бөркін айналдыра бастады. Кәдімгі «жолбасар» ұрыларша шапан-бөркін айналдырып киіп, мұрны мен аузын қызыл орамалмен таңып алып, Жұмабайды қуғанда тағы сол ұрыларша, «даусымды танытпаймын» деп, мыңқылдап сөйлеп бұйрық берген. Байтас қорықса, қорықпаса да сыр алдырған жоқ Сондықтан, Жұмабайдың ашуын алыстан танып, мәз болып күліп келе\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 10, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2058db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мен қазір астарында жазылып келе жатыр. Сонымен тағы тірлігінің қайтты десе, жөнін бір барлық болатын. Оның барын болғанын еститін деп болсын, сөзін жөндеп алып, бір екеуіне онша бұл жиын осылай жарқапты. Әйгерім екеуі қара тұсынан ойланғандай боп, қоныс жеңілген байын. Соны да алдында барын бастаған-ды.\n",
      "Бұл жүз бала екі шайсыз таңғаны қозылысқан бақылды соның жеті ашылуы бойынан тағы бір күн құйдырып\n"
     ]
    }
   ],
   "source": [
    "# from utils import sample\n",
    "\n",
    "context = \"Мен \"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 400, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56670d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сен аулы күңкетпестен төр жүз алып\n"
     ]
    }
   ],
   "source": [
    "context = \"Сен \"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 30, temperature=1, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
